{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, down_layer_num=4, bottom_layer_num=2, up_layer_num=4, start_channel=1, next_channel=64, final_channel=2, conv_ker=3, conv_st=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.layers = {}\n",
    "        self.down_layer_num = down_layer_num\n",
    "        self.bottom_layer_num = bottom_layer_num\n",
    "        self.up_layer_num = up_layer_num\n",
    "        \n",
    "        for i in range(1, down_layer_num+1):\n",
    "            self.layers[\"down_conv_\"+str(i)] = nn.Sequential(self.ConvBatchnorm2dReLU(start_channel, next_channel, conv_ker, conv_st),\n",
    "                                                             self.ConvBatchnorm2dReLU(next_channel, next_channel, conv_ker, conv_st))\n",
    "            self.layers[\"down_maxpool_\"+str(i)] = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            start_channel = next_channel\n",
    "            next_channel *= next_channel\n",
    "        \n",
    "        for i in range(1, bottom_layer_num+1):\n",
    "            self.layers[\"bottom_conv_\"+str(i)] = self.ConvBatchnorm2dReLU(start_channel, next_channel, conv_ker, conv_st)\n",
    "            if i == 1:\n",
    "                start_channel = next_channel\n",
    "        \n",
    "        for i in range(1, up_layer_num+1):\n",
    "            next_channel //= 2\n",
    "            self.layers[\"up_upconv_\"+str(i)] = nn.ConvTranspose2d(start_channel, next_channel, kernel_size=2, stride=2)\n",
    "            self.layers[\"up_conv_\"+str(i)] = nn.Sequential(self.ConvBatchnorm2dReLU(start_channel, next_channel, conv_ker, conv_st),\n",
    "                                                           self.ConvBatchnorm2dReLU(next_channel, next_channel, conv_ker, conv_st))\n",
    "            start_channel = next_channel\n",
    "        \n",
    "        self.layers[\"final_conv1x1\"] = nn.Conv2d(start_channel, final_channel, kernel_size=1, stride=1)\n",
    "        # self.down_conv_2_1 = self.ConvBatchnorm2dReLU(64, 128, 3, 1)\n",
    "        # self.down_conv_2_2 = self.ConvBatchnorm2dReLU(128, 128, 3, 1)\n",
    "        # self.down_maxpool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # self.down_conv_3_1 = self.ConvBatchnorm2dReLU(128, 256, 3, 1)\n",
    "        # self.down_conv_3_2 = self.ConvBatchnorm2dReLU(256, 256, 3, 1)\n",
    "        # self.down_maxpool_3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # self.down_conv_4_1 = self.ConvBatchnorm2dReLU(256, 512, 3, 1)\n",
    "        # self.down_conv_4_2 = self.ConvBatchnorm2dReLU(512, 512, 3, 1)\n",
    "        # self.down_maxpool_4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # self.bottom_conv_1 = self.ConvBatchnorm2dReLU(512, 1024, 3, 1)\n",
    "        # self.bottom_conv_2 = self.ConvBatchnorm2dReLU(1024, 1024, 3, 1)\n",
    "        \n",
    "        # self.up_upconv_1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        \n",
    "    def ConvBatchnorm2dReLU(self, in_channels, out_channels, kernel_size, stride):\n",
    "        layers = nn.Sequential([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_input = {} # for residual connection\n",
    "        \n",
    "        for i in range(1, self.down_layer_num+1):\n",
    "            x = self.layers[\"down_conv_\"+str(i)](x)\n",
    "            res_input[\"down_conv_\"+str(i)] = x\n",
    "            x = self.layers[\"down_maxpool_\"+str(i)](x)\n",
    "        \n",
    "        for i in range(1, self.bottom_layer_num+1):\n",
    "            x = self.layers[\"bottom_conv_\"+str(i)](x)\n",
    "        \n",
    "        for i in range(1, self.up_layer_num+1):\n",
    "            x = self.layers[\"up_upconv_\"+str(i)](x)\n",
    "            x = torch.cat(transforms.CentorCrop(x.shape[2:])(res_input[\"down_conv_\"+str(self.down_layer_num+1-i)]), x, dim=1) # batch!\n",
    "            x = self.layers[\"up_conv_\"+str(i)](x)\n",
    "        \n",
    "        x = self.layers[\"final_conv1x1\"](x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
